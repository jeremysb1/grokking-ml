{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOylbtnwkXVqApq5kH5DsAC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeremysb1/grokking-ml/blob/main/adaline_algo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adaptive linear neurons and the convergence of learning.\n",
        "\n",
        "I am taking a look at a type of single-layer neural network (NN): ADAptive LInear NEuron (Adaline).\n",
        "\n",
        "The Adaline algorithm is particularly interesting because it illustrates the key concepts of defining and minimizing continuous loss functions.\n",
        "\n",
        "## Minimizing loss functions with gradient decent\n",
        "\n",
        "One of the key ingredients of supervised machine learning algorithms is a defined objective function that is to be optimized during the learning process. This objective function is often a loss or cost function that we want to minimize.\n",
        "\n",
        "In the case of Adaline, we can define the loss function, L, to learn the model parameters as the mean squared error (MSE) between the calculated outcome and the true class label.\n",
        "\n"
      ],
      "metadata": {
        "id": "bZGZ4qRfjGxv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OQqn14nmi2sz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AdalineGD:\n",
        "\n",
        "  \"\"\"ADAptive LInear NEuron classifier.\n",
        "\n",
        "    Parameters\n",
        "    ------------\n",
        "    eta : float\n",
        "        Learning rate (between 0.0 and 1.0)\n",
        "    n_iter : int\n",
        "        Passes over the training dataset.\n",
        "    random_state : int\n",
        "        Random number generator seed for random weight initialization.\n",
        "\n",
        "    Attributes\n",
        "    -----------\n",
        "    w_ : 1d-array\n",
        "        Weights after fitting.\n",
        "    b_ : Scalar\n",
        "        Bias unit after fitting.\n",
        "    losses_ : list\n",
        "      Mean squared error loss function values in each epoch.\n",
        "    \"\"\"\n",
        "\n",
        "  def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
        "    self.eta = eta\n",
        "    self.n_iter = n_iter\n",
        "    self.random_state = random_state\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    \"\"\" Fit training data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like}, shape = [n_examples, n_features]\n",
        "            Training vectors, where n_examples\n",
        "            is the number of examples and\n",
        "            n_features is the number of features.\n",
        "        y : array-like, shape = [n_examples]\n",
        "            Target values.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "    \"\"\"\n",
        "\n",
        "    rgen = np.random.RandomState(self.random_state)\n",
        "    self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])\n",
        "    self.b_ = np.float_(0.)\n",
        "    self.losses_ = []\n",
        "\n",
        "    for i in range(self.n_iter):\n",
        "      net_input = self.net_input(X)\n",
        "      output = self.activation(net_input)\n",
        "      errors = (y - output)\n",
        "      self.w_ += self.eta * 2.0 * X.T.dot(errors) / X.shape[0]\n",
        "      self.b_ += self.eta * 2.0 * errors.mean()\n",
        "      loss = (errors ** 2).mean()\n",
        "      self.losses_.append(loss)\n",
        "    return self\n",
        "\n",
        "  def net_input(self, X):\n",
        "    return np.dot(X, self.w_) + self.b_\n",
        "\n",
        "  def activation(self, X):\n",
        "    return X\n",
        "\n",
        "  def predict(self, X):\n",
        "    return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L_LfMIM5leuB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Large scale machine learning and stochastic gradient descent\n"
      ],
      "metadata": {
        "id": "cZYyx6o09lhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdalineSGD:\n",
        "    \"\"\"ADAptive LInear NEuron classifier.\n",
        "\n",
        "    Parameters\n",
        "    ------------\n",
        "    eta : float\n",
        "        Learning rate (between 0.0 and 1.0)\n",
        "    n_iter : int\n",
        "        Passes over the training dataset.\n",
        "    shuffle : bool (default: True)\n",
        "        Shuffles training data every epoch if True to prevent\n",
        "        cycles.\n",
        "    random_state : int\n",
        "        Random number generator seed for random weight\n",
        "        initialization.\n",
        "\n",
        "\n",
        "    Attributes\n",
        "    -----------\n",
        "    w_ : 1d-array\n",
        "        Weights after fitting.\n",
        "    b_ : Scalar\n",
        "        Bias unit after fitting.\n",
        "    losses_ : list\n",
        "        Mean squared error loss function value averaged over all\n",
        "        training examples in each epoch.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, eta=0.01, n_iter=10,\n",
        "                 shuffle=True, random_state=None):\n",
        "        self.eta = eta\n",
        "        self.n_iter = n_iter\n",
        "        self.w_initialized = False\n",
        "        self.shuffle = shuffle\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\" Fit training data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like}, shape = [n_examples, n_features]\n",
        "            Training vectors, where n_examples is the number of\n",
        "            examples and n_features is the number of features.\n",
        "        y : array-like, shape = [n_examples]\n",
        "            Target values.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "\n",
        "        \"\"\"\n",
        "        self._initialize_weights(X.shape[1])\n",
        "        self.losses_ = []\n",
        "        for i in range(self.n_iter):\n",
        "            if self.shuffle:\n",
        "                X, y = self._shuffle(X, y)\n",
        "            losses = []\n",
        "            for xi, target in zip(X, y):\n",
        "                losses.append(self._update_weights(xi, target))\n",
        "            avg_loss = np.mean(losses)\n",
        "            self.losses_.append(avg_loss)\n",
        "        return self\n",
        "\n",
        "    def partial_fit(self, X, y):\n",
        "        \"\"\"Fit training data without reinitializing the weights\"\"\"\n",
        "        if not self.w_initialized:\n",
        "            self._initialize_weights(X.shape[1])\n",
        "        if y.ravel().shape[0] > 1:\n",
        "            for xi, target in zip(X, y):\n",
        "                self._update_weights(xi, target)\n",
        "        else:\n",
        "            self._update_weights(X, y)\n",
        "        return self\n",
        "\n",
        "    def _shuffle(self, X, y):\n",
        "        \"\"\"Shuffle training data\"\"\"\n",
        "        r = self.rgen.permutation(len(y))\n",
        "        return X[r], y[r]\n",
        "\n",
        "    def _initialize_weights(self, m):\n",
        "        \"\"\"Initialize weights to small random numbers\"\"\"\n",
        "        self.rgen = np.random.RandomState(self.random_state)\n",
        "        self.w_ = self.rgen.normal(loc=0.0, scale=0.01,\n",
        "                                   size=m)\n",
        "        self.b_ = np.float_(0.)\n",
        "        self.w_initialized = True\n",
        "\n",
        "    def _update_weights(self, xi, target):\n",
        "        \"\"\"Apply Adaline learning rule to update the weights\"\"\"\n",
        "        output = self.activation(self.net_input(xi))\n",
        "        error = (target - output)\n",
        "        self.w_ += self.eta * 2.0 * xi * (error)\n",
        "        self.b_ += self.eta * 2.0 * error\n",
        "        loss = error**2\n",
        "        return loss\n",
        "\n",
        "    def net_input(self, X):\n",
        "        \"\"\"Calculate net input\"\"\"\n",
        "        return np.dot(X, self.w_) + self.b_\n",
        "\n",
        "    def activation(self, X):\n",
        "        \"\"\"Compute linear activation\"\"\"\n",
        "        return X\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Return class label after unit step\"\"\"\n",
        "        return np.where(self.activation(self.net_input(X))\n",
        "                        >= 0.5, 1, 0)\n"
      ],
      "metadata": {
        "id": "IEKRQn5vBv99"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}